{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules Import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section, we import all necessary libraries for our data processing and analysis tasks. This includes libraries for handling data structures (pandas, numpy), visualization (matplotlib, seaborn), working with JSON and compressed files (json, gzip), and text processing (nltk, re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnltk.download('vader_lexicon')\\nnltk.download('stopwords')\\nnltk.download('punkt')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import gzip\n",
    "import ast\n",
    "import re\n",
    "import dask.dataframe as dd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Stop words download\n",
    "'''\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we load and preprocess the dataset containing information about various games. This involves reading from a compressed .gz file, decoding JSON objects, and cleaning the data by dropping rows that are completely null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_games(file_path_games):\n",
    "\n",
    "    \"\"\"\n",
    "    Load and process a dataset of game information stored in a JSON file.\n",
    "\n",
    "    Args:\n",
    "    file_path_games (str): The file path to the dataset in JSON format, with each game's data on a separate line.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A processed DataFrame with selected game information and derived features.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    - Reads a JSON line file into a DataFrame.\n",
    "    - Removes rows and columns that contain only missing values or irrelevant information.\n",
    "    - Converts the 'price' column to numeric, handling non-numeric and specific strings.\n",
    "    - Parses and extracts the year from the 'release_date' and handles non-standard entries.\n",
    "    - Processes the 'genres' into dummy variables and removes infrequent genre columns.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    df_ga = []\n",
    "    with open(file_path_games, 'r') as file:\n",
    "        # Load each line of the file as a separate JSON object and append to a list\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            df_ga.append(json_obj)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df_ga = pd.DataFrame(df_ga)\n",
    "\n",
    "    # Drop rows where all elements are NaN and remove specific columns\n",
    "    df_ga.dropna(how='all', inplace=True)\n",
    "    df_ga.drop(columns=['publisher', 'title', 'url', 'early_access', 'reviews_url', 'tags'], inplace=True)\n",
    "    \n",
    "    # Convert the 'price' field, replace 'Free to Play' with 0 and coerce errors\n",
    "    df_ga['price'] = pd.to_numeric(df_ga['price'].replace('Free to Play', 0), errors='coerce')\n",
    "    \n",
    "    # Parse 'release_date', handle missing and specific string values, extract year\n",
    "    df_ga['release_date'] = pd.to_datetime(df_ga['release_date'].fillna(0).replace('Soon..', 0), errors='coerce')\n",
    "    df_ga['release_date'] = df_ga['release_date'].dt.year.astype('Int64')\n",
    "\n",
    "    # Ensure 'genres' is a list, and create dummy variables for each genre\n",
    "    df_ga['genres'] = df_ga['genres'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    for genre in set([genre for sublist in df_ga['genres'] for genre in sublist]):\n",
    "        df_ga[f'genre_{genre}'] = df_ga['genres'].apply(lambda x: 1 if genre in x else 0)\n",
    "\n",
    "    # Remove original 'genres' column\n",
    "    df_ga.drop(columns=['genres'], inplace=True)\n",
    "\n",
    "    # Remove genre columns with fewer than 100 occurrences\n",
    "    for column in df_ga.columns:\n",
    "        if column.startswith('genre_') and df_ga[column].sum() < 100:\n",
    "            df_ga.drop(columns=[column], inplace=True)\n",
    "\n",
    "    return df_ga\n",
    "\n",
    "# Usage example:\n",
    "file_path_games = r\"..\\PI MLOps - STEAM\\steam_games.json\\output_steam_games.json\"\n",
    "df_ga = load_and_process_games(file_path_games)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de ayuda para el procesamiento de texto y anÃ¡lisis de sentimientos\n",
    "def preprocess_text(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocesses a given text by converting to lowercase, removing URLs and special characters,\n",
    "    tokenizing, removing stopwords, and applying stemming.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text string that needs to be processed.\n",
    "\n",
    "    Returns:\n",
    "    str: The processed text string.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    - Converts the text to lowercase to standardize it.\n",
    "    - Removes URLs to clean the text from any web links.\n",
    "    - Removes any non-alphabetic characters (special characters and numbers) to focus on words.\n",
    "    - Tokenizes the text into individual words or tokens.\n",
    "    - Filters out common English stopwords to reduce noise and focuses on meaningful words.\n",
    "    - Applies stemming to reduce words to their root form, enabling basic normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Handle the case where the input text is None\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert text to lowercase to ensure uniformity\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs from the text to clean unnecessary web links\n",
    "    text = re.sub(r'http\\S+', '', text) \n",
    "\n",
    "    # Remove non-alphabetic characters to focus on words only\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) \n",
    "\n",
    "    # Tokenize the text into individual words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and apply stemming to each word\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in set(stopwords.words('english'))]\n",
    "\n",
    "    # Join the tokens back into a single string separated by space\n",
    "    return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_vader(sid, text):\n",
    "\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of a given text using VADER and classify it as positive, neutral, or negative.\n",
    "\n",
    "    Args:\n",
    "    sid (SentimentIntensityAnalyzer): The VADER SentimentIntensityAnalyzer instance.\n",
    "    text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "    int: The sentiment classification result (2 for positive, 1 for neutral, 0 for negative).\n",
    "\n",
    "    This function performs the following operations:\n",
    "    - Checks if the input text is missing (NaN) and returns 1 (neutral) if so.\n",
    "    - Computes the sentiment scores using VADER's polarity_scores method.\n",
    "    - Extracts the 'compound' score, which is a normalized, weighted composite score.\n",
    "    - Classifies the sentiment based on the 'compound' score:\n",
    "      - Positive if the compound score is 0.05 or higher.\n",
    "      - Negative if the compound score is -0.05 or lower.\n",
    "      - Neutral otherwise (between -0.05 and 0.05).\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for missing text and return neutral if no text is provided\n",
    "    if pd.isna(text):\n",
    "        return 1  # Neutral sentiment for missing text\n",
    "    \n",
    "    # Obtain polarity scores for the text from VADER\n",
    "    scores = sid.polarity_scores(text)\n",
    "    compound = scores['compound'] # Extract the compound score\n",
    "\n",
    "    # Determine sentiment classification based on the compound score\n",
    "    if compound >= 0.05:\n",
    "        return 2  # Positive sentiment\n",
    "    elif compound <= -0.05:\n",
    "        return 0  # Negative sentiment\n",
    "    else:\n",
    "        return 1  # Neutral sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value_from_dict(series, key):\n",
    "\n",
    "    \"\"\"\n",
    "    Extract a value from a dictionary inside a pandas Series based on the provided key.\n",
    "\n",
    "    Args:\n",
    "    series (pd.Series): A pandas Series containing dictionary objects.\n",
    "    key (str): The key for which the value needs to be extracted.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A Series containing the extracted values for the specified key.\n",
    "    \"\"\"\n",
    "        \n",
    "    return series.apply(lambda x: x.get(key) if isinstance(x, dict) else None)\n",
    "\n",
    "\n",
    "def load_and_process_reviews(file_path_reviews):\n",
    "\n",
    "    \"\"\"\n",
    "    Load and process review data from a JSON file, extract relevant fields, apply text preprocessing,\n",
    "    perform sentiment analysis, and filter the data based on specific criteria.\n",
    "\n",
    "    Args:\n",
    "    file_path_reviews (str): The file path to the reviews dataset in JSON format.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A processed DataFrame containing cleaned reviews and their sentiment analysis results.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    - Reads a JSON file line by line and parses the data.\n",
    "    - Drops unnecessary columns and handles missing values.\n",
    "    - Extracts specific fields from a nested dictionary within the 'reviews' column.\n",
    "    - Filters out reviews based on text length.\n",
    "    - Applies text preprocessing and sentiment analysis.\n",
    "    - Cleans the DataFrame by removing duplicates and irrelevant rows.\n",
    "    \"\"\"\n",
    "\n",
    "    df_re = []\n",
    "    with open(file_path_reviews, 'r', encoding='utf-8') as file:\n",
    "        # Load each line of the file and convert it to a dictionary\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_data = ast.literal_eval(line)\n",
    "                df_re.append(json_data)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error in line: {line}\")\n",
    "                continue\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df_re = pd.DataFrame(df_re)\n",
    "\n",
    "    # Drop columns and rows with all elements missing\n",
    "    df_re.drop(['user_url'], axis=1, inplace=True)\n",
    "    df_re.dropna(how='all', inplace=True)\n",
    "\n",
    "    # Explode 'reviews' column to expand lists into rows and reset index\n",
    "    df_re = df_re.explode('reviews').reset_index(drop=True)\n",
    "    \n",
    "    # Extract relevant information from the 'reviews' dictionaries\n",
    "    df_re['item_id'] = extract_value_from_dict(df_re['reviews'], 'item_id')\n",
    "    df_re['recommend'] = extract_value_from_dict(df_re['reviews'], 'recommend')\n",
    "    df_re['review_text'] = extract_value_from_dict(df_re['reviews'], 'review')\n",
    "\n",
    "    # Drop rows where 'review_text' is missing\n",
    "    df_re.dropna(subset=['review_text'], inplace=True)\n",
    "\n",
    "    # Filter reviews to include only those with at least 5 words\n",
    "    df_re = df_re[df_re['review_text'].apply(lambda x: len(x.split()) >= 5)]\n",
    "\n",
    "    # Remove the original 'reviews' column and any duplicates\n",
    "    df_re.drop(columns=['reviews'], inplace=True)\n",
    "    df_re.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Perform sentiment analysis on cleaned reviews\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df_re['cleaned_review'] = df_re['review_text'].apply(preprocess_text)\n",
    "    df_re['sentiment_analysis'] = df_re['cleaned_review'].apply(lambda text: sentiment_analysis_vader(sid, text))\n",
    "\n",
    "    # Drop rows where 'sentiment_analysis' results are missing\n",
    "    df_re.dropna(subset=['sentiment_analysis'], inplace=True)\n",
    "\n",
    "    return df_re\n",
    "\n",
    "# File path for the JSON dataset\n",
    "file_path_reviews = r\"..\\PI MLOps - STEAM\\user_reviews.json\\australian_user_reviews.json\"\n",
    "\n",
    "# Processing and storing the reviews dataset\n",
    "df_re = load_and_process_reviews(file_path_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarly, for the reviews dataset, we process each line from a JSON file, handling potential errors and converting lines into a list of dictionaries, which we then transform into a DataFrame. This step is crucial for structuring the reviews data for further analysis.\n",
    "\n",
    "#### In order to analyze the sentiment of user reviews, we first preprocess the text to remove URLs, special characters, and numbers, and to tokenize the text. This preprocessing step is vital for reducing noise in the text data and improving the performance of our sentiment analysis.\n",
    "\n",
    "#### We utilize the VADER tool from the nltk library to perform sentiment analysis on the preprocessed review texts. This tool is particularly suited for texts from social media and similar contexts due to its sensitivity to both the polarity and intensity of emotions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items Dateset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section, we focus on the dataset that contains information about users' items. This includes the games or software owned by users on the Steam platform. We start by loading the data from a JSON file and proceed to clean it by removing entries that don't provide meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_items(file_path_items):\n",
    "\n",
    "    \"\"\"\n",
    "    Load and process item data from a JSON file, extract relevant fields, and filter the data based on specific criteria.\n",
    "\n",
    "    Args:\n",
    "    file_path_items (str): The file path to the items dataset in JSON format.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A processed DataFrame containing item data with filters applied to playtime.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    - Reads a JSON file line by line and parses the data into a list of dictionaries.\n",
    "    - Converts the list into a DataFrame and drops unnecessary columns.\n",
    "    - Filters out entries with a zero item count and expands lists of items into separate rows.\n",
    "    - Extracts relevant information from the nested dictionaries within the 'items' column.\n",
    "    - Filters items based on the 'playtime_forever' to include only those with less than 2 hours.\n",
    "    - Removes the original 'items' column and any duplicate entries.\n",
    "\n",
    "    The function is particularly useful for cleaning and preparing game user data for analysis,\n",
    "    focusing on user engagement measured through playtime.\n",
    "    \"\"\"\n",
    "\n",
    "    df_it = []\n",
    "    with open(file_path_items, 'r', encoding='utf-8') as file:\n",
    "        # Load each line of the file, parse as a dictionary, and append to a list\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_data = ast.literal_eval(line)\n",
    "                df_it.append(json_data)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error in line: {line}\")\n",
    "                continue\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df_it = pd.DataFrame(df_it)\n",
    "\n",
    "    # Drop columns and filter rows where 'items_count' is zero\n",
    "    df_it.drop(['user_url'], axis=1, inplace=True)\n",
    "    df_it = df_it[df_it['items_count'] != 0].reset_index(drop=True)\n",
    "\n",
    "    # Explode 'items' column to separate rows and reset index\n",
    "    df_it = df_it.explode('items').reset_index(drop=True)\n",
    "    \n",
    "    # Extract 'item_id' and 'playtime_forever' from the dictionaries in the 'items' column\n",
    "    df_it['item_id'] = df_it['items'].apply(lambda x: x.get('item_id') if isinstance(x, dict) else None)\n",
    "    df_it['playtime_forever'] = df_it['items'].apply(lambda x: x.get('playtime_forever') if isinstance(x, dict) else None)\n",
    "\n",
    "    # Filter items to include only those with less than 2 hours of playtime\n",
    "    df_it = df_it[df_it['playtime_forever'] < 7200]    \n",
    "    \n",
    "    # Remove the original 'items' column and any duplicates\n",
    "    df_it.drop(columns=['items'], inplace=True)\n",
    "    df_it.drop_duplicates(inplace=True)\n",
    "   \n",
    "    return df_it\n",
    "\n",
    "\n",
    "# File path for the JSON dataset\n",
    "file_path_items = r\"..\\PI MLOps - STEAM\\users_items.json\\australian_users_items.json\"\n",
    "\n",
    "# Processing and storing the items dataset\n",
    "df_it = load_and_process_items(file_path_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precalculated Datesets for API feeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_developer_stats(df_ga):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate statistics for each developer in the dataset, including the total number of games and the percentage of games that are free.\n",
    "\n",
    "    Args:\n",
    "    df_ga (pd.DataFrame): A DataFrame containing game data with at least 'developer', 'id', and 'price' columns.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    - Groups the data by the 'developer' column.\n",
    "    - Aggregates two statistics for each developer:\n",
    "      * The total number of games developed.\n",
    "      * The count of games that are free (price == 0).\n",
    "    - Calculates the percentage of games that are free for each developer.\n",
    "    - Saves the resulting statistics DataFrame to a Parquet file for efficient storage and access.\n",
    "\n",
    "    The use of Parquet format is ideal for performance in both storage and computational efficiency, especially when dealing with large datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Group the DataFrame by 'developer' and aggregate the necessary statistics\n",
    "    developer_stats = df_ga.groupby('developer').agg(\n",
    "        total_games=('id', 'count'), # Count the number of games per developer\n",
    "        free_games=('price', lambda x: (x == 0).sum()) # Count how many games are free\n",
    "    )\n",
    "\n",
    "    # Calculate the percentage of free games for each developer\n",
    "    developer_stats['free_game_percentage'] = (developer_stats['free_games'] / developer_stats['total_games']) * 100\n",
    "\n",
    "    # Save the aggregated data to a Parquet file\n",
    "    developer_stats.to_parquet ('../datasets/developer_stats.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_review_analysis(df_ga, df_re):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate and save a summary of sentiment analysis results for each developer based on reviews of their games.\n",
    "\n",
    "    Args:\n",
    "    df_ga (pd.DataFrame): A DataFrame containing game data with at least 'id' and 'developer' columns.\n",
    "    df_re (pd.DataFrame): A DataFrame containing review data with at least 'item_id' and 'sentiment_analysis' columns.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    - Merges review data with game data to associate reviews with the developers of the games.\n",
    "    - Aggregates sentiment analysis results by developer.\n",
    "    - Saves the aggregated data to a Parquet file for efficient storage and access.\n",
    "\n",
    "    The analysis groups sentiments into categories (e.g., positive, neutral, negative) and counts occurrences for each category per developer. This allows quick retrieval and analysis of sentiment data for each developer, facilitating user feedback analysis at a granular level.\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge review data with game data to map each review to its respective developer\n",
    "    df_re = df_re.merge(df_ga[['id', 'developer']], left_on='item_id', right_on='id')\n",
    "\n",
    "    # Aggregate sentiment analysis results by developer and fill missing values with 0 (for developers with no reviews in a particular category)\n",
    "    review_analysis = df_re.groupby('developer')['sentiment_analysis'].value_counts().unstack().fillna(0)\n",
    "\n",
    "    # Save the results to a Parquet file\n",
    "    review_analysis.to_parquet('../datasets/review_analysis.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculo de datos para Endpoint 3\n",
    "def precalculate_genre_playtime_stats(df_it, df_ga):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate and save average playtime statistics for each genre and overall, using game item and game data.\n",
    "\n",
    "    Args:\n",
    "    df_it (pd.DataFrame): A DataFrame containing item data with at least 'item_id' and 'playtime_forever' columns.\n",
    "    df_ga (pd.DataFrame): A DataFrame containing game data with 'id' and genre-specific columns.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    - Identifies genre columns in the game data.\n",
    "    - Merges item data with game data to link playtime with game genres.\n",
    "    - Computes the average playtime for each genre and the overall average playtime across all genres.\n",
    "    - Saves the results to a Parquet file for efficient storage and access.\n",
    "\n",
    "    These statistics provide insights into user engagement across different game genres, facilitating targeted marketing and game development strategies.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify genre columns in the game DataFrame\n",
    "    genre_columns = [col for col in df_ga.columns if col.startswith('genre_')]\n",
    "\n",
    "    # Merge the item and game DataFrames to correlate items with their game genre\n",
    "    merged_df = df_it.merge(df_ga, left_on='item_id', right_on='id')\n",
    "\n",
    "    # Initialize a dictionary to store playtime statistics for each genre\n",
    "    genre_playtime_stats = {}\n",
    "\n",
    "    # Calculate the average playtime for each genre\n",
    "    for genre_col in genre_columns:\n",
    "        # Only consider rows where the genre is applicable\n",
    "        average_playtime = merged_df[merged_df[genre_col] == 1]['playtime_forever'].mean()\n",
    "        genre = genre_col.split('_')[1] # Extract the genre name from the column\n",
    "        genre_playtime_stats[genre] = average_playtime\n",
    "\n",
    "    # Calculate the overall average playtime across all genres\n",
    "    total_average_playtime = merged_df['playtime_forever'].mean()\n",
    "\n",
    "    # Convert the dictionary of genre playtimes to a DataFrame\n",
    "    genre_playtime_df = pd.DataFrame.from_dict(genre_playtime_stats, orient='index', columns=['average_playtime'])\n",
    "    genre_playtime_df['total_average_playtime'] = total_average_playtime\n",
    "\n",
    "    # Save the DataFrame as a Parquet file for efficient storage and querying\n",
    "    genre_playtime_df.to_parquet('../datasets/genre_playtime_stats.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculo de datos para Endpoint 4\n",
    "def precalculate_user_data(df_ga, df_re, df_it):\n",
    "\n",
    "    \"\"\"\n",
    "    Aggregate user data across multiple datasets to calculate spending, review counts, and recommendation percentages.\n",
    "\n",
    "    Args:\n",
    "    df_ga (pd.DataFrame): DataFrame containing game data, specifically game ids and prices.\n",
    "    df_re (pd.DataFrame): DataFrame containing review data, specifically item ids and recommendations.\n",
    "    df_it (pd.DataFrame): DataFrame containing user-item interactions, including user ids and item ids.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    - Converts the Pandas DataFrames to Dask DataFrames to utilize parallel processing for handling larger data volumes.\n",
    "    - Merges these DataFrames to align user-item interactions with game prices and reviews.\n",
    "    - Aggregates data to calculate the total spent, total reviews, and number of recommendations per user.\n",
    "    - Computes the recommendation percentage for each user.\n",
    "    - Saves the aggregated user data to a Parquet file for efficient storage and querying.\n",
    "\n",
    "    The use of Dask allows for efficient computation on larger datasets that might not fit into memory if using Pandas alone.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert Pandas DataFrames to Dask DataFrames for parallel processing\n",
    "    ddf_it = dd.from_pandas(df_it, npartitions=15)\n",
    "    \n",
    "    # Merge the dataframes using Dask to handle large datasets\n",
    "    df_merged = ddf_it.merge(df_ga[['id', 'price']], left_on='item_id', right_on='id', how='left')\n",
    "    df_merged = df_merged.merge(df_re[['item_id', 'recommend']], on='item_id', how='left')\n",
    "    \n",
    "    # Aggregate data using Dask, which is optimized for big data processing\n",
    "    df_user_stats = df_merged.groupby('user_id').agg({\n",
    "        'price': 'sum',\n",
    "        'recommend': 'count',\n",
    "        'item_id': 'count'\n",
    "    }).compute()  # Trigger computations and bring results back to Pandas for further operations\n",
    "\n",
    "    # Rename columns for clarity and calculate the recommendation percentage\n",
    "    df_user_stats = df_user_stats.rename(columns={\n",
    "        'price': 'total_spent',\n",
    "        'recommend': 'recommendations',\n",
    "        'item_id': 'total_reviews'\n",
    "    })\n",
    "    df_user_stats['recommendation_percentage'] = (df_user_stats['recommendations'] / df_user_stats['total_reviews']) * 100\n",
    "    \n",
    "    # Save the resulting DataFrame to a Parquet\n",
    "    df_user_stats.to_parquet('../datasets/user_data.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculo de datos para Endpoint 5\n",
    "def precalculate_best_developers_by_year(df_ga, df_re):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate and save the most popular game developers by year based on the number of positive reviews.\n",
    "\n",
    "    Args:\n",
    "    df_ga (pd.DataFrame): DataFrame containing game data, specifically game ids, developers, and release dates.\n",
    "    df_re (pd.DataFrame): DataFrame containing review data, specifically item ids and sentiment analysis results.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    - Merges game data with review data to correlate games with their reviews.\n",
    "    - Filters for positive reviews only (where sentiment_analysis == 2).\n",
    "    - Aggregates the count of positive reviews by developer and release year.\n",
    "    - Identifies the developer with the highest number of positive reviews for each year.\n",
    "    - Saves this information in a Parquet file for efficient storage and quick access.\n",
    "\n",
    "    This data can be particularly useful for marketing analysis, trend tracking, and recognizing industry leaders.\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge game and review datasets to align reviews with corresponding game details\n",
    "    combined_df = df_ga.merge(df_re, left_on='id', right_on='item_id')\n",
    "\n",
    "    # Filter for positive reviews only\n",
    "    positive_reviews = combined_df[combined_df['sentiment_analysis'] == 2]\n",
    "\n",
    "    # Count positive reviews per developer and release year\n",
    "    top_devs_by_year = positive_reviews.groupby(['developer', 'release_date']).size()\n",
    "\n",
    "    # Identify the developer with the most positive reviews each year\n",
    "    top_devs_by_year = top_devs_by_year.reset_index(name='positive_reviews')\n",
    "    top_devs_by_year = top_devs_by_year.sort_values(['release_date', 'positive_reviews'], ascending=False)\n",
    "    top_devs_by_year = top_devs_by_year.groupby('release_date').first().reset_index()\n",
    "\n",
    "    # Ensure the correct order of columns\n",
    "    top_devs_by_year = top_devs_by_year[['release_date', 'developer']]\n",
    "\n",
    "    # Save the result to a Parquet file\n",
    "    top_devs_by_year.to_parquet(f'../datasets/best_developers_by_year.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_recommendation_data(df_ga, df_re, df_it):\n",
    "    \"\"\"\n",
    "    Create a recommendation dataset by merging game, review, and user-item interaction data,\n",
    "    and aggregating necessary information for recommendation purposes.\n",
    "\n",
    "    Args:\n",
    "    df_ga (pd.DataFrame): DataFrame containing game data with game IDs and other game-related attributes.\n",
    "    df_re (pd.DataFrame): DataFrame containing review data with game IDs and sentiment analysis results.\n",
    "    df_it (pd.DataFrame): DataFrame containing user-item interaction data, including playtime.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    - Converts Pandas DataFrames to Dask DataFrames for efficient parallel processing.\n",
    "    - Merges these DataFrames to align games with their reviews and user-item interactions.\n",
    "    - Filters and retains columns of interest for the recommendation system, such as game genres and user activities.\n",
    "    - Groups data by game titles to aggregate genre data and compute average playtime.\n",
    "    - Saves the aggregated data to a Parquet file with GZIP compression for efficient storage and retrieval.\n",
    "\n",
    "    The function is designed to provide a structured dataset that supports building or improving game recommendation systems.\n",
    "    \"\"\"\n",
    "    # Convert Pandas DataFrames to Dask DataFrames for parallel processing\n",
    "    ddf_ga = dd.from_pandas(df_ga, npartitions=10)\n",
    "    ddf_re = dd.from_pandas(df_re, npartitions=10)\n",
    "    ddf_it = dd.from_pandas(df_it, npartitions=102)\n",
    "\n",
    "    # Merge DataFrames to combine game data with reviews and user interactions\n",
    "    ddf_combined = ddf_ga.merge(ddf_re, left_on='id', right_on='item_id', how='left')\n",
    "    ddf_combined = ddf_combined.merge(ddf_it, left_on='id', right_on='item_id', how='left')\n",
    "\n",
    "    # Define columns of interest for the recommendation system\n",
    "    columns_of_interest = ['id', 'app_name'] + \\\n",
    "                          [col for col in df_ga.columns if 'genre_' in col] + \\\n",
    "                          ['user_id', 'sentiment_analysis', 'playtime_forever']\n",
    "    columns_to_keep = [col for col in columns_of_interest if col in ddf_combined.columns]\n",
    "    ddf_combined = ddf_combined[columns_to_keep]\n",
    "\n",
    "    # Aggregate genre data and calculate average playtime by game title\n",
    "    ddf_grouped = ddf_combined.groupby('app_name').sum()\n",
    "    ddf_grouped['avg_playtime_forever'] = ddf_combined.groupby('app_name')['playtime_forever'].mean()\n",
    "\n",
    "    # Repartition the DataFrame to a single partition for output\n",
    "    ddf_grouped = ddf_grouped.repartition(npartitions=1)\n",
    "\n",
    "    # Save the processed data to a Parquet file, using GZIP compression\n",
    "    ddf_grouped.to_parquet('../datasets/recommendation_dataset.parquet', write_index=False, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This section of the code is responsible for invoking a series of precalculation functions that have been defined above.\n",
    "Each function processes and aggregates data across different aspects of game analytics to prepare datasets for advanced analysis and operations. \n",
    "The functions being called are:\n",
    "- precalculate_developer_stats: Processes and saves developer-specific game statistics.\n",
    "- precalculate_review_analysis: Analyzes and aggregates review data by developers.\n",
    "- precalculate_genre_playtime_stats: Calculates and stores playtime statistics by game genre.\n",
    "- precalculate_user_data: Aggregates comprehensive user data from game interactions and reviews.\n",
    "- precalculate_best_developers_by_year: Determines and stores the most successful game developers of each year based on review data.\n",
    "- precalculate_recommendation_data: Creates a dataset optimized for game recommendation algorithms.\n",
    "\n",
    "Each function is designed to enhance data accessibility and analytical readiness, feeding into subsequent analytical tasks or systems.\n",
    "'''\n",
    "\n",
    "precalculate_developer_stats(df_ga)\n",
    "precalculate_review_analysis(df_ga, df_re)\n",
    "precalculate_genre_playtime_stats(df_it, df_ga)\n",
    "precalculate_user_data(df_ga, df_re, df_it)\n",
    "precalculate_best_developers_by_year(df_ga, df_re)\n",
    "#precalculate_recommendation_data(df_ga, df_re, df_it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
